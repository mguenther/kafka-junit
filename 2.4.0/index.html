<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<!--[if IE]><meta http-equiv="X-UA-Compatible" content="IE=edge"><![endif]-->
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 1.5.4">
<meta name="author" content="Markus Günther">
<title>User Guide to Kafka for JUnit</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<link rel="stylesheet" href="css//asciidoctor.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.5.0/css/font-awesome.min.css">
</head>
<body class="book toc2 toc-left">
<div id="header">
<h1>User Guide to Kafka for JUnit</h1>
<div class="details">
<span id="author" class="author">Markus Günther</span><br>
<span id="revnumber">version 2.4.0,</span>
<span id="revdate">2020-02-07</span>
</div>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#section:introduction">1. Introduction</a></li>
<li><a href="#_using_kafka_for_junit_in_your_tests">2. Using Kafka for JUnit in your tests</a>
<ul class="sectlevel2">
<li><a href="#_using_junit_4_rules">2.1. Using JUnit 4 rules</a></li>
<li><a href="#_what_about_junit_5">2.2. What about JUnit 5?</a></li>
<li><a href="#_alternative_ways">2.3. Alternative ways</a></li>
<li><a href="#_supported_versions_of_apache_kafka">2.4. Supported versions of Apache Kafka</a></li>
</ul>
</li>
<li><a href="#section:embedded-kafka-cluster">3. Working with an embedded Kafka cluster</a>
<ul class="sectlevel2">
<li><a href="#_failure_modes">3.1. Failure Modes</a></li>
</ul>
</li>
<li><a href="#section:external-kafka-cluster">4. Working with an external Kafka cluster</a></li>
<li><a href="#section:producing-records">5. Producing records</a>
<ul class="sectlevel2">
<li><a href="#_sending_non_keyed_values_using_defaults">5.1. Sending non-keyed values using defaults</a></li>
<li><a href="#_sending_non_keyed_values_using_overrides">5.2. Sending non-keyed values using overrides</a></li>
<li><a href="#_sending_non_keyed_values_transactionally">5.3. Sending non-keyed values transactionally</a></li>
<li><a href="#_sending_keyed_records_using_defaults">5.4. Sending keyed records using defaults</a></li>
<li><a href="#_sending_keyed_records_using_overrides">5.5. Sending keyed records using overrides</a></li>
<li><a href="#_sending_keyed_records_transactionally">5.6. Sending keyed records transactionally</a></li>
<li><a href="#_sending_records_or_values_transactionally_to_multiple_topics">5.7. Sending records or values transactionally to multiple topics</a></li>
<li><a href="#_failing_a_transaction_on_purpose">5.8. Failing a transaction on purpose</a></li>
<li><a href="#_attaching_record_headers">5.9. Attaching record headers</a></li>
</ul>
</li>
<li><a href="#section:consuming-records">6. Consuming records</a>
<ul class="sectlevel2">
<li><a href="#_consuming_values_using_defaults">6.1. Consuming values using defaults</a></li>
<li><a href="#_consuming_key_value_based_records_using_defaults">6.2. Consuming key-value based records using defaults</a></li>
<li><a href="#_consuming_key_value_based_records_using_overrides">6.3. Consuming key-value based records using overrides</a></li>
<li><a href="#_working_with_attached_headers">6.4. Working with attached headers</a></li>
<li><a href="#_consuming_values_or_records_transactionally">6.5. Consuming values or records transactionally</a></li>
<li><a href="#_observing_a_topic_until_code_n_code_values_have_been_consumed">6.6. Observing a topic until <code>N</code> values have been consumed</a></li>
<li><a href="#_observing_a_topic_until_code_n_code_records_have_been_consumed">6.7. Observing a topic until <code>N</code> records have been consumed</a></li>
<li><a href="#_using_key_filters_when_consuming_or_observing_a_topic">6.8. Using key filters when consuming or observing a topic</a></li>
<li><a href="#_using_value_filters_when_consuming_or_observing_a_topic">6.9. Using value filters when consuming or observing a topic</a></li>
<li><a href="#_using_header_filters_when_consuming_or_observing_a_topic">6.10. Using header filters when consuming or observing a topic</a></li>
<li><a href="#_obtaining_metadata_per_record">6.11. Obtaining metadata per-record</a></li>
<li><a href="#_seeking_to_a_dedicated_offset_of_a_topic_partition">6.12. Seeking to a dedicated offset of a topic-partition</a></li>
</ul>
</li>
<li><a href="#section:managing-topics">7. Managing topics</a>
<ul class="sectlevel2">
<li><a href="#_creating_a_topic">7.1. Creating a topic</a></li>
<li><a href="#_deleting_a_topic">7.2. Deleting a topic</a></li>
<li><a href="#_determine_whether_a_topic_exists">7.3. Determine whether a topic exists</a></li>
<li><a href="#_retrieving_the_leader_and_the_in_sync_replica_set_isr">7.4. Retrieving the leader and the In-Sync-Replica Set (ISR)</a></li>
<li><a href="#_retrieving_the_topic_configuration_remotely">7.5. Retrieving the topic configuration remotely</a></li>
</ul>
</li>
<li><a href="#section:colophon">8. License</a></li>
</ul>
</div>
</div>
<div id="content">
<div class="sect1">
<h2 id="section:introduction">1. Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Kafka for JUnit provides JUnit 4.x rule implementations that enables developers to start and stop a complete Kafka cluster comprised of Kafka brokers and distributed Kafka Connect workers from within a JUnit test. It also provides a rich set of convenient accessors to interact with such an embedded Kafka cluster in a lean and non-obtrusive way.</p>
</div>
<div class="paragraph">
<p>Kafka for JUnit can be used to both whitebox-test individual Kafka-based components of your application or to blackbox-test applications that offer an incoming and/or outgoing Kafka-based interface.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_using_kafka_for_junit_in_your_tests">2. Using Kafka for JUnit in your tests</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Kafka for JUnit provides the necessary infrastructure to exercise your Kafka-based components against an embeddable Kafka cluster (cf. <a href="#section:embedded-kafka-cluster">Working with an embedded Kafka cluster</a>). However, Kafka for JUnit got you covered as well if you are simply interested in using the convenient accessors against Kafka clusters that are already present in your infrastructure (cf. section <a href="#section:external-kafka-cluster">Working with an external Kafka cluster</a>).</p>
</div>
<div class="sect2">
<h3 id="_using_junit_4_rules">2.1. Using JUnit 4 rules</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">public class KafkaTest {

  @Rule
  public EmbeddedKafkaCluster cluster = provisionWith(useDefaults());

  @Test
  public void shouldWaitForRecordsToBePublished() throws Exception {
    cluster.send(to("test-topic", "a", "b", "c").useDefaults());
    cluster.observe(on("test-topic", 4).useDefaults());
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The same applies for <code>@ClassRule</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_what_about_junit_5">2.2. What about JUnit 5?</h3>
<div class="paragraph">
<p>You can use Kafka for JUnit with JUnit 5 of course. However, with its rule-based implementations, Kafka for JUnit is currently tailored for ease of use with JUnit 4. It implements no JUnit Jupiter extension for JUnit 5. There is an issue for that (cf. <a href="https://github.com/mguenther/kafka-junit/issues/4">ISSUE-004</a>), so the development wrt. a JUnit Jupiter extension is planned for a future release. PRs are welcome, though!
As Junit 5 does not support rules, one approach is to start your cluster in a <code>@BeforeEach</code> method and it stop in an <code>@AfterEach</code> method.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">public class JUnit5KafkaTest {

    private EmbeddedKafkaCluster cluster;

    @BeforeEach
    public void setupKafka() {
        cluster = provisionWith(useDefaults());
        cluster.start();
    }

    @AfterEach
    public void tearDownKafka() {
        cluster.stop();
    }

    @Test
    public void shouldWaitForRecordsToBePublished() throws Exception {
        cluster.send(to("test-topic", "a", "b", "c").useDefaults());
        cluster.observe(on("test-topic", 4).useDefaults());
    }

}</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_alternative_ways">2.3. Alternative ways</h3>
<div class="paragraph">
<p>You do not have to use the JUnit 4 rules if you are not comfortable with them. <code>EmbeddedKafkaCluster</code> implements the <code>AutoCloseable</code> interface, so it is easy to manage it inside your tests yourself.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">public class KafkaTest {

  @Test
  public void shouldWaitForRecordsToBePublished() throws Exception {

    try (EmbeddedKafkaCluster cluster = provisionWith(useDefaults())) {
      cluster.start();
      cluster.send(to("test-topic", "a", "b", "c").useDefaults());
      cluster.observe(on("test-topic", 3).useDefaults());
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Of course, you can also test against existing clusters using <code>ExternalKafkaCluster</code> instead of <code>EmbeddableKafkaCluster</code>. See section <a href="#section:external-kafka-cluster">Working with an external Kafka cluster</a> for more information.</p>
</div>
</div>
<div class="sect2">
<h3 id="_supported_versions_of_apache_kafka">2.4. Supported versions of Apache Kafka</h3>
<table class="tableblock frame-all grid-all spread">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Version of Kafka for JUnit</th>
<th class="tableblock halign-left valign-top">Supports</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.1.x</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Apache Kafka 1.0.0</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.2.x</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Apache Kafka 1.0.0</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.3.x</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Apache Kafka 1.0.0</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">1.0.x</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Apache Kafka 1.1.1</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">2.0.x</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Apache Kafka 2.0.0</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">2.1.x</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Apache Kafka 2.1.1</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">2.2.x</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Apache Kafka 2.2.1</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">2.3.x</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Apache Kafka 2.3.0</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">2.4.x</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Apache Kafka 2.4.0</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="section:embedded-kafka-cluster">3. Working with an embedded Kafka cluster</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Kafka for JUnit is able to spin up a fully-fledged embedded Kafka cluster that is accessible via class <code>EmbeddedKafkaCluster</code>. <code>EmbeddedKafkaCluster</code> implements the interfaces <code>RecordProducer</code>, <code>RecordConsumer</code> and <code>TopicManager</code> and thus provides convenient accessors to interact with the cluster.</p>
</div>
<div class="paragraph">
<p>Using <code>EmbeddedKafkaCluster</code> in a JUnit test is quite simple. The necessary code to set it up is minimal if you are comfortable with the default configuration.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">import static net.mguenther.kafka.junit.EmbeddedKafkaCluster.provisionWith;
import static net.mguenther.kafka.junit.EmbeddedKafkaClusterConfig.useDefaults;

public class MyTest {

    @Rule
    public EmbeddedKafkaCluster cluster = provisionWith(useDefaults());
}</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>EmbeddedKafkaCluster</code> is a JUnit rule (it is derived from <code>ExternalResource</code> to be precise). The <code>@Rule</code> annotation ties the lifecycle to the execution of the test method. The <code>@ClassRule</code> annotation ties the lifecycle of <code>EmbeddedKafkaCluster</code> to the execution of the test class. In both cases, all acquired resources are released automatically once the resp. lifecycle terminates.</p>
</div>
<div class="paragraph">
<p>The example underneath demonstrates how to use <code>@ClassRule</code> instead of <code>@Rule</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">import static net.mguenther.kafka.junit.EmbeddedKafkaCluster.provisionWith;
import static net.mguenther.kafka.junit.EmbeddedKafkaClusterConfig.useDefaults;

public class MyTest {

    @ClassRule
    public static EmbeddedKafkaCluster cluster = provisionWith(useDefaults());
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Kafka for JUnit uses the Builder pattern extensively to provide a fluent API when provisioning an embedded Kafka cluster. Let&#8217;s take a closer look at method <code>EmbeddedKafkaCluster.provisionWith</code>. This method consumes a configuration of type <code>EmbeddedKafkaClusterConfig</code>. <code>EmbeddedKafkaClusterConfig</code> uses defaults for the Kafka broker and ZooKeeper. By default, Kafka Connect will not be provisioned at all. The builder of <code>EmbeddedKafkaClusterConfig</code> provides a <code>provisionWith</code> method as well and is overloaded to accept configurations of type <code>EmbeddedZooKeeperConfig</code>, <code>EmbeddedKafkaConfig</code> and <code>EmbeddedConnectConfig</code>. The following listing demonstrates how to adjust the configuration of the embedded Kafka broker wrt. the default number of partitions for newly created topics.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">import static net.mguenther.kafka.junit.EmbeddedKafkaCluster.provisionWith;

public class MyTest {
    @Rule
    public EmbeddedKafkaCluster cluster = provisionWith(EmbeddedKafkaClusterConfig
            .create()
            .provisionWith(EmbeddedKafkaConfig
                .create()
                .with(KafkaConfig$.MODULE$.NumPartitionsProp(), "5")
                .build())
            .build());
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The builders for those configurations provide a uniform interface for overriding defaults, comprising two methods <code>with(String propertyName, T value)</code> and <code>withAll(java.util.Properties overrides)</code>. To override a default value, you simply provide the name of the configuration parameter as defined by the resp. Kafka component along with the new value.</p>
</div>
<div class="paragraph">
<p>Using the default setting will provide you with a single embedded Kafka broker. This ought to be sufficient for most cases. However, there are scenarios which require testing against multiple brokers that form a cluster. Forming an embedded cluster with multiple brokers is done by adjusting the default provisioning of your test case. See the listing underneath for an example.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">public class MultipleBrokersTest {

    @Rule
    public EmbeddedKafkaCluster cluster = provisionWith(EmbeddedKafkaClusterConfig.create()
            .provisionWith(EmbeddedKafkaConfig.create()
                .withNumberOfBrokers(3)
                .with(KafkaConfig$.MODULE$.NumPartitionsProp(), "5")
                .with(KafkaConfig$.MODULE$.DefaultReplicationFactorProp(), "3")
                .with(KafkaConfig$.MODULE$.MinInSyncReplicasProp(), "2")
                .with(KafkaConfig$.MODULE$.OffsetsTopicReplicationFactorProp(), "3")
                .with(KafkaConfig$.MODULE$.TransactionsTopicReplicationFactorProp(), "3")
                .with(KafkaConfig$.MODULE$.TransactionsTopicMinISRProp(), "2")
                .build())
            .build());
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Using this configuration, we end up with a total of three brokers that form an embedded Kafka cluster, while the defaults for topic partitions and replicas have been adjusted to be consistent with the size of the cluster.</p>
</div>
<div class="paragraph">
<p>See sections on <a href="#section:producing-records">Producing records</a>, <a href="#section:consuming-records">Consuming records</a> and <a href="#section:managing-topics">Managing topics</a> for further reference on how to interact with the cluster.</p>
</div>
<div class="sect2">
<h3 id="_failure_modes">3.1. Failure Modes</h3>
<div class="paragraph">
<p><code>EmbeddedKafkaCluster</code> provides the means to disconnect - and re-connect of course - specific embedded Kafka brokers. All brokers in the embedded cluster get broker ID assigned during cluster formation. This broker ID is an <code>Integer</code>-based value and starts at 1. The broker ID increases by 1 for every subsequent embedded Kafka broker that is started during cluster formation.</p>
</div>
<div class="paragraph">
<p>Clusters stay fixed wrt. the maximum number of embedded brokers. But individual brokers can, given their broker ID, be disconnected from the rest of the cluster to test for failure scenarios. Such failure scenarios include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>How does may Kafka-based component behave in the presence of broker outages?</p>
</li>
<li>
<p>What happens if the In-Sync-Replica Set (ISR) of a topic that my application consumes from shrinks below its minimum size?</p>
</li>
<li>
<p>Is my application able to progress after brokers re-connect and form a working cluster?</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_disconnect_and_reconnect_a_single_broker">3.1.1. Disconnect and reconnect a single broker</h4>
<div class="paragraph">
<p>The following listing shows how to disconnect and re-connect a certain broker, while fetching the ISR of a dedicated topic in between these operations to determine whether the cluster behaves correctly.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you do use this feature of Kafka for JUnit, then please give the embedded cluster some time to handle broker churn. Identifying that a leader for a topic-partition is not available and conducting the leader election takes some time. In the example underneath we introduce a delay of five seconds in between operations that affect cluster membership.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">cluster.createTopic(TopicConfig.forTopic("test-topic")
    .withNumberOfPartitions(5)
    .withNumberOfReplicas(3)
    .build());

delay(5);

Set&lt;Integer&gt; leaders = cluster.fetchLeaderAndIsr("test-topic")
    .values()
    .stream()
    .map(LeaderAndIsr::getLeader)
    .collect(Collectors.toSet());

assertThat(leaders.contains(1)).isTrue();
assertThat(leaders.contains(2)).isTrue();
assertThat(leaders.contains(3)).isTrue();

cluster.disconnect(1);

delay(5);

Set&lt;Integer&gt; leadersAfterDisconnect = cluster.fetchLeaderAndIsr("test-topic")
    .values()
    .stream()
    .map(LeaderAndIsr::getLeader)
    .collect(Collectors.toSet());

assertThat(leadersAfterDisconnect.contains(1)).isFalse();
assertThat(leadersAfterDisconnect.contains(2)).isTrue();
assertThat(leadersAfterDisconnect.contains(3)).isTrue();

cluster.connect(1);

delay(5);

Set&lt;Integer&gt; leadersAfterReconnect = cluster.fetchLeaderAndIsr("test-topic")
    .values()
    .stream()
    .map(LeaderAndIsr::getLeader)
    .collect(Collectors.toSet());

assertThat(leadersAfterReconnect.contains(1)).isTrue();
assertThat(leadersAfterReconnect.contains(2)).isTrue();
assertThat(leadersAfterReconnect.contains(3)).isTrue();</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_disconnect_until_in_sync_replica_set_falls_below_minimum_size">3.1.2. Disconnect until In-Sync-Replica Set falls below minimum size</h4>
<div class="paragraph">
<p>The following listing shows how to disconnect the In-Sync-Replica Set (ISR) for a given topic until its ISR falls below its minimum size.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you do use this feature of Kafka for JUnit, then please give the embedded cluster some time to handle broker churn. Identifying that a leader for a topic-partition is not available and conducting the leader election takes some time. In the example underneath we introduce a delay of five seconds in between operations that affect cluster membership.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">// Create a topic and configure the number of replicas as well as the size of the ISR

cluster.createTopic(TopicConfig.forTopic("test-topic")
    .withNumberOfPartitions(5)
    .withNumberOfReplicas(3)
    .with("min.insync.replicas", "2")
    .build());

// Wait a bit to give the cluster a chance to properly assign topic-partitions to leaders

delay(5);

// Disconnect until the remaining number of brokers fall below the minimum ISR size

cluster.disconnectUntilIsrFallsBelowMinimumSize("test-topic");

delay(5);

// Submitting records to this topic will yield a NotEnoughReplicasException

cluster.send(SendValues.to("test-topic", "A").useDefaults());</code></pre>
</div>
</div>
<div class="paragraph">
<p>The last line of the listing shows the effect of an ISR that can no longer operate reliably. Your Kafka-based component or application would run concurrently to this test so that you are able to observe if it behaves correctly (e.g. by checking that the component progresses normally if the ISR is restored).</p>
</div>
</div>
<div class="sect3">
<h4 id="_restoring_the_in_sync_replica_set">3.1.3. Restoring the In-Sync-Replica Set</h4>
<div class="paragraph">
<p>Restoring the In-Sync-Replica Set is easy, as method <code>disconnectUntilIsrFallsBelowMinimumSize</code> returns a list of broker IDs for all brokers that have been deactivated during the shrinking. The following listing shows how to restore the ISR.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">cluster.createTopic(TopicConfig.forTopic("test-topic")
    .withNumberOfPartitions(5)
    .withNumberOfReplicas(3)
    .with("min.insync.replicas", "2")
    .build());

delay(5);

Set&lt;Integer&gt; disconnectedBrokers = cluster.disconnectUntilIsrFallsBelowMinimumSize("test-topic");

delay(5);

// Do some testing, trigger some operations, observe the behavior of your application

cluster.connect(disconnectedBrokers);

// Give the cluster some time to assign leaders and reestablish the ISR

delay(5);

// Do some more testing ...</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="section:external-kafka-cluster">4. Working with an external Kafka cluster</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Kafka for JUnit can be used to work with an external Kafka cluster. This is useful if you want to execute your tests against a staging/testing environment or if you already use other testing libraries (e.g. Testcontainers) that spin up a Kafka cluster on your local machine, but want to use the convenient accessors provided by Kafka for JUnit.</p>
</div>
<div class="paragraph">
<p>Class <code>ExternalKafkaCluster</code> integrates an external cluster. Just like <code>EmbeddableKafkaCluster</code>, an <code>ExternalKafkaCluster</code> also implements the interfaces <code>RecordProducer</code>, <code>RecordConsumer</code> and <code>TopicManager</code> and thus provides convenient accessors to interact with the cluster.</p>
</div>
<div class="paragraph">
<p>Using <code>ExternalKafkaCluster</code> in a JUnit test is easy. The listing below shows the necessary code to use <code>ExternalKafkaCluster</code> in combination with Testcontainers.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">public class MyTest {

    // This is not part of Kafka for JUnit, but a sub-module provided
    // by Testcontainers (org.testcontainers:kafka)
    @Rule
    public KafkaContainer kafkaContainer = new KafkaContainer();

    @Test
    public void externalKafkaClusterDemo() throws Exception {

        ExternalKafkaCluster cluster = ExternalKafkaCluster.at(kafkaContainer.getBootstrapServers());

        // use the accessors that cluster provides to interact with the Kafka container
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>See sections on <a href="#section:producing-records">Producing records</a>, <a href="#section:consuming-records">Consuming records</a> and <a href="#section:managing-topics">Managing topics</a> for further reference on how to interact with the cluster.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="section:producing-records">5. Producing records</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Class <code>EmbeddedKafkaClusterRule</code> as well as <code>EmbeddedKafkaCluster</code> expose convenience methods for producing new Kafka records. Have a look at the <code>RecordProducer</code> interface (Javadoc omitted for brevity).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">public interface RecordProducer {

    &lt;V&gt; List&lt;RecordMetadata&gt; send(SendValues&lt;V&gt; sendRequest) throws InterruptedException;
    &lt;V&gt; List&lt;RecordMetadata&gt; send(SendValuesTransactional&lt;V&gt; sendRequest) throws InterruptedException;
    &lt;K, V&gt; List&lt;RecordMetadata&gt; send(SendKeyValues&lt;K, V&gt; sendRequest) throws InterruptedException;
    &lt;K, V&gt; List&lt;RecordMetadata&gt; send(SendKeyValuesTransactional&lt;K, V&gt; sendRequest) throws InterruptedException;
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Calling <code>send</code> using an instance of <code>SendValues</code> does just that: It produces un-keyed Kafka records that only feature a value. The key of a record that has been produced this way is simply <code>null</code>.  If you wish to associate a key, you can do so by passing an instance of <code>SendKeyValues</code> to the <code>send</code> method. Both <code>SendValues</code> and <code>SendKeyValues</code> use the <a href="https://en.wikipedia.org/wiki/Builder_pattern">Builder pattern</a> so that creating the resp. send parameterization is easy and does not pollute your test code with any kind of boilerplate.</p>
</div>
<div class="paragraph">
<p>Implementations of the <code>RecordProducer</code> interface use the high-level producer API that comes with Apache Kafka. Hence, the underlying producer is a <code>KafkaProducer</code>. This <code>KafkaProducer</code> is fully parameterizable via the builders of both <code>SendValues</code> and <code>SendKeyValues</code>.</p>
</div>
<div class="paragraph">
<p>All <code>send</code> operations are executed <strong>synchronously</strong>.</p>
</div>
<div class="paragraph">
<p>With these abstractions in place, sending content to your embedded Kafka cluster is easy. Have a look at the following examples . One thing you should notice is that you do not have to specify <code>bootstrap.servers</code>. Kafka for JUnit adjusts a given client configuration so that you can start off with meaningful defaults that work out-of-the-box. You&#8217;ll only have to provide configuration overrides if it is absolutely necessary for your test.</p>
</div>
<div class="sect2">
<h3 id="_sending_non_keyed_values_using_defaults">5.1. Sending non-keyed values using defaults</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">SendValues&lt;String&gt; sendRequest = SendValues.to("test-topic", "a", "b", "c").useDefaults();
cluster.send(sendRequest);</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_sending_non_keyed_values_using_overrides">5.2. Sending non-keyed values using overrides</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">SendValues&lt;String&gt; sendRequest = SendValues.to("test-topic", "a", "b", "c")
        .with(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true")
        .with(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, "1")
        .build();

cluster.send(sendRequest);</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_sending_non_keyed_values_transactionally">5.3. Sending non-keyed values transactionally</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">SendValuesTransactional&lt;String&gt; sendRequest = SendValuesTransactional
        .inTransaction("test-topic", Arrays.asList("a", "b", "c"))
        .useDefaults()

cluster.send(sendRequest);</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The API of Kafka for JUnit has been designed with great care and readability in mind. Using <code>static</code> imports for factory methods shows that we can interact with the embedded Kafka cluster in a lean and readable way.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">cluster.send(inTransaction("test-topic", Arrays.asList("a", "b", "c")).useDefaults());</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_sending_keyed_records_using_defaults">5.4. Sending keyed records using defaults</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">List&lt;KeyValue&lt;String, String&gt;&gt; records = new ArrayList&lt;&gt;();

records.add(new KeyValue&lt;&gt;("aggregate", "a"));
records.add(new KeyValue&lt;&gt;("aggregate", "b"));
records.add(new KeyValue&lt;&gt;("aggregate", "c"));

SendKeyValues&lt;String, String&gt; sendRequest = SendKeyValues.to("test-topic", records).useDefaults();

cluster.send(sendRequest);</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_sending_keyed_records_using_overrides">5.5. Sending keyed records using overrides</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">List&lt;KeyValue&lt;String, String&gt;&gt; records = new ArrayList&lt;&gt;();

records.add(new KeyValue&lt;&gt;("aggregate", "a"));
records.add(new KeyValue&lt;&gt;("aggregate", "b"));
records.add(new KeyValue&lt;&gt;("aggregate", "c"));

SendKeyValues&lt;String, String&gt; sendRequest = SendKeyValues.to("test-topic", records)
        .with(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true")
        .with(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, "1")
        .build();

cluster.send(sendRequest);</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_sending_keyed_records_transactionally">5.6. Sending keyed records transactionally</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">List&lt;KeyValue&lt;String, String&gt;&gt; records = new ArrayList&lt;&gt;();

records.add(new KeyValue&lt;&gt;("aggregate", "a"));
records.add(new KeyValue&lt;&gt;("aggregate", "b"));
records.add(new KeyValue&lt;&gt;("aggregate", "c"));

cluster.send(inTransaction("test-topic", records).useDefaults());</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_sending_records_or_values_transactionally_to_multiple_topics">5.7. Sending records or values transactionally to multiple topics</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">SendValuesTransactional&lt;String&gt; sendRequest = SendValuesTransactional
        .inTransaction("test-topic-1", Arrays.asList("a", "b"))
        .inTransaction("test-topic-2", Arrays.asList("c", "d"))
        .useDefaults();

cluster.send(sendRequest);</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_failing_a_transaction_on_purpose">5.8. Failing a transaction on purpose</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">SendValuesTransactional&lt;String&gt; sendRequest = SendValuesTransactional
        .inTransaction("test-topic", Arrays.asList("a", "b"))
        .failTransaction()
        .build();

cluster.send(sendRequest);</code></pre>
</div>
</div>
<div class="paragraph">
<p>Defining a <code>SendValuesTransactional</code> request with <code>failTransaction</code> will write records to the Kafka log, but abort the transaction they belong to. This allows you to test if your application-specific Kafka consumers adhere to the transactional guarantees they claim to satisfy, since only a correct implementation of a consumer with <code>isolation.level</code> set to <code>read_committed</code> must see - and process - those records.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This works for <code>SendKeyValuesTransactional</code> as well.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_attaching_record_headers">5.9. Attaching record headers</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">KeyValue&lt;String, String&gt; record = new KeyValue&lt;&gt;("a", "b");
record.addHeader("client", "kafka-junit-test".getBytes("utf-8"));

SendKeyValues&lt;String, String&gt; sendRequest = SendKeyValues
        .to("test-topic", Collections.singletonList(record))
        .useDefaults();

cluster.send(sendRequest);</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You can also pre-construct an instance of <code>Headers</code> and pass it along via the constructor of a <code>KeyValue</code>.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="section:consuming-records">6. Consuming records</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Class <code>EmbeddedKafkaClusterRule</code> as well as <code>EmbeddedKafkaCluster</code> expose convenience methods for consuming Kafka records. Have a look at the <code>RecordConsumer</code> interface (Javadoc omitted for brevity).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">public interface RecordConsumer {

    &lt;V&gt; List&lt;V&gt; readValues(ReadKeyValues&lt;String, V&gt; readRequest);
    &lt;V&gt; List&lt;V&gt; observeValues(ObserveKeyValues&lt;String, V&gt; observeRequest) throws InterruptedException;
    &lt;K, V&gt; List&lt;KeyValue&lt;K, V&gt;&gt; read(ReadKeyValues&lt;K, V&gt; readRequest);
    &lt;K, V&gt; List&lt;KeyValue&lt;K, V&gt;&gt; observe(ObserveKeyValues&lt;K, V&gt; observeRequest) throws InterruptedException;
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Implementations of the <code>RecordConsumer</code> interface use the high-level consumer API that comes with Apache Kafka. Hence, the underlying consumer is a <code>KafkaConsumer</code>. This <code>KafkaConsumer</code> is fully parameterizable via both <code>ReadKeyValues</code> and <code>ObserveKeyValues</code> by means of <code>kafka.consumer.ConsumerConfig</code>.</p>
</div>
<div class="paragraph">
<p>All operations are executed <strong>synchronously</strong>.</p>
</div>
<div class="paragraph">
<p>With these abstractions in place, reading content from a Kafka topic is easy. As with a <code>RecordProducer</code>, there is no need to specify things like <code>bootstrap.servers</code> - Kafka for JUnit will provide the necessary configuration. Have a look at the following examples.</p>
</div>
<div class="sect2">
<h3 id="_consuming_values_using_defaults">6.1. Consuming values using defaults</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">ReadKeyValues&lt;String, String&gt; readRequest = ReadKeyValues.from("test-topic").useDefaults();

List&lt;String&gt; values = cluster.readValues(readRequest);</code></pre>
</div>
</div>
<div class="paragraph">
<p>By default, <code>ReadKeyValues.from</code> uses <code>StringDeserializer.class</code> for both the record key and value. Calling <code>readValues</code> just yields the values from the consumed Kafka records. Please have a look at the next example if you are interested in obtaining not only the values, but also the record key and possibly attached headers.</p>
</div>
</div>
<div class="sect2">
<h3 id="_consuming_key_value_based_records_using_defaults">6.2. Consuming key-value based records using defaults</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">ReadKeyValues&lt;String, String&gt; readRequest = ReadKeyValues.from("test-topic").useDefaults();

List&lt;KeyValue&lt;String, String&gt;&gt; consumedRecords = cluster.read(readRequest);</code></pre>
</div>
</div>
<div class="paragraph">
<p>Notice the difference to the example before this one: Instead of calling <code>readValues</code> we call <code>read</code> using the same <code>ReadKeyValues</code> request. Instead of a <code>List&lt;String&gt;</code>, this yields a <code>List&lt;KeyValue&lt;String, String&gt;&gt;</code> where each <code>KeyValue</code> is comprised of the record key, the record value and the headers that have been attached to that record.</p>
</div>
</div>
<div class="sect2">
<h3 id="_consuming_key_value_based_records_using_overrides">6.3. Consuming key-value based records using overrides</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">ReadKeyValues&lt;String, Long&gt; readRequest = ReadKeyValues.from("test-topic-value-types", Long.class)
        .with(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, LongDeserializer.class)
        .build();

List&lt;KeyValue&lt;String, Long&gt;&gt; consumedRecords = cluster.read(readRequest);</code></pre>
</div>
</div>
<div class="paragraph">
<p>Since we are interested in consuming records that use <code>Long</code>-based values, we have to parameterize the <code>ReadKeyValues</code> request such that the proper type is bound and a compatible <code>Deserializer</code> is used. This is all done by calling <code>from(String, Class&lt;V&gt;)</code> and overriding the default deserializer using <code>with(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, LongDeserializer.class)</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_working_with_attached_headers">6.4. Working with attached headers</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">Headers headersOfFirstRecord = cluster.read(ReadKeyValues.from("test-topic").useDefaults())
        .stream()
        .findFirst()
        .map(KeyValue::getHeaders)
        .orElseThrow(() -&gt; new RuntimeException("No records found."));</code></pre>
</div>
</div>
<div class="paragraph">
<p>The example grabs the <code>Headers</code> of the first record that it reads. <code>Headers</code> is a class that comes from the Kafka Client API. See its <a href="https://kafka.apache.org/10/javadoc/org/apache/kafka/common/header/Headers.html">Javadoc</a> for a thorough explanation of its public interface.</p>
</div>
</div>
<div class="sect2">
<h3 id="_consuming_values_or_records_transactionally">6.5. Consuming values or records transactionally</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">ReadKeyValues&lt;String, String&gt; readRequest = ReadKeyValues.from("test-topic")
    .with(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed")
    .build();

List&lt;String&gt; consumedValues = cluster.readValues(readRequest);</code></pre>
</div>
</div>
<div class="paragraph">
<p>Consuming records that have been written transactionally is just a matter of the configuration of the underlying <code>KafkaConsumer</code>. As <code>ReadKeyValues</code> provides full access to the configuration of the <code>KafkaConsumer</code> a simple <code>with(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed")</code> suffices to enable transactional consume semantics.</p>
</div>
</div>
<div class="sect2">
<h3 id="_observing_a_topic_until_code_n_code_values_have_been_consumed">6.6. Observing a topic until <code>N</code> values have been consumed</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">ObserveKeyValues&lt;String, String&gt; observeRequest = ObserveKeyValues.on("test-topic", 3).useDefaults();

List&lt;String&gt; observedValues = cluster.observeValues(observeRequest);</code></pre>
</div>
</div>
<div class="paragraph">
<p>Sometimes you are not interested in reading all available values from a topic, but want to block test execution until a certain amount of values have been read from a topic. This enables you to synchronize your test logic with the system-under-test as the test blocks until the system-under-test has been able to write its records to the Kafka topic you are currently observing.</p>
</div>
<div class="paragraph">
<p>Of course, observing a topic cannot run indefinitely and thus has to be parameterized with a timeout. There is a default timeout which should be sensible for most usage scenarios. However, if you need to observe a topic for a longer amount of time, you can easily parameterize the <code>ObserveKeyValues</code> request using <code>observeFor(int, TimeUnit)</code>.</p>
</div>
<div class="paragraph">
<p>If the timeout elapses before the desired amount of values have been read from the given topic, the <code>observe</code> method will throw an <code>AssertionError</code>. Hence, if you are just interested in the fact that records have been written by the system-under-test to the topic you are observing, it fully suffices to use</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">cluster.observeValues(observeRequest);</code></pre>
</div>
</div>
<div class="paragraph">
<p>and let the timeout elapse to fail the test.</p>
</div>
<div class="paragraph">
<p>If you are interested in the observed values, you can however simply grab all records - like shown above - and perform additional assertions on them.</p>
</div>
</div>
<div class="sect2">
<h3 id="_observing_a_topic_until_code_n_code_records_have_been_consumed">6.7. Observing a topic until <code>N</code> records have been consumed</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">ObserveKeyValues&lt;String, String&gt; observeRequest = ObserveKeyValues.on("test-topic", 3).useDefaults();

List&lt;KeyValue&lt;String, String&gt;&gt; observedValues = cluster.observe(observeRequest);</code></pre>
</div>
</div>
<div class="paragraph">
<p>This is just the same as the example above, but instead of observing and returning <code>List&lt;String&gt;</code> it returns a <code>List&lt;KeyValue&lt;String, String&gt;&gt;</code> in the example.</p>
</div>
</div>
<div class="sect2">
<h3 id="_using_key_filters_when_consuming_or_observing_a_topic">6.8. Using key filters when consuming or observing a topic</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">Predicate&lt;String&gt; keyFilter = k -&gt; Integer.parseInt(k) % 2 == 0;

ReadKeyValues&lt;String, Integer&gt; readRequest = ReadKeyValues.from("test-topic", Integer.class)
        .with(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class)
        .filterOnKeys(keyFilter)
        .build();

List&lt;KeyValue&lt;String, Integer&gt;&gt; consumedRecords = cluster.read(readRequest);</code></pre>
</div>
</div>
<div class="paragraph">
<p>It is possible to parameterize both <code>ReadKeyValues</code> and <code>ObserveKeyValues</code> with a key filter. The key filter is modelled using <code>java.util.function.Predicate</code> and thus can be arbitrarily complex. The default key filter evaluates to <code>true</code> every time, so unless you do not explicitly provide a filter using <code>filterOnKeys</code> like in the example, all consumed records pass the filter.</p>
</div>
<div class="paragraph">
<p>In this example, the filter is quite simply and parses the key of the record into an <code>Integer</code> and checks if it is evenly divisible by 2. So, if - for the sake of the example - the topic from which we read contains the keys <code>"1"</code>, <code>"2"</code>, <code>"3"</code> and <code>"4"</code>, only those records with keys <code>"2"</code> and <code>"4"</code> would pass the filter.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Applying a key filter is also possible when observing a topic.
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Combining key, value and header filters is possible. Please note that in this case only such records that pass all filters are returned. Hence, conjoining key, value and header filters has <code>AND</code> semantics.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_using_value_filters_when_consuming_or_observing_a_topic">6.9. Using value filters when consuming or observing a topic</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">Predicate&lt;Integer&gt; valueFilter = v -&gt; v &gt; 2;

ReadKeyValues&lt;String, Integer&gt; readRequest = ReadKeyValues.from("test-topic", Integer.class)
        .with(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class)
        .filterOnValues(valueFilter)
        .build();

List&lt;KeyValue&lt;String, Integer&gt;&gt; consumedRecords = cluster.read(readRequest);</code></pre>
</div>
</div>
<div class="paragraph">
<p>It is possible to parameterize both <code>ReadKeyValues</code> and <code>ObserveKeyValues</code> with a value filter. Like the key filter, the value filter is also modelled using <code>java.util.function.Predicate</code>. The default value filter evaluates to <code>true</code> every time, so unless you do not explicitly provide a filter using <code>filterOnValues</code> like in the example, all consumed records pass the filter.</p>
</div>
<div class="paragraph">
<p>In this example, the filter only lets those records pass for which the associated <code>Integer</code>-based record value is larger than 2. So, if the topic holds records with values <code>1</code>, <code>2</code> and <code>3</code>, only the record with value <code>3</code> would pass the filter.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Applying a value filter is also possible when observing a topic.
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Combining key, value and header filters is possible. Please note that in this case only such records that pass all filters are returned. Hence, conjoining key, value and header filters has <code>AND</code> semantics.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_using_header_filters_when_consuming_or_observing_a_topic">6.10. Using header filters when consuming or observing a topic</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">Predicate&lt;Headers&gt; headersFilter = headers -&gt; new String(headers.lastHeader("aggregate").value()).equals("a");

ReadKeyValues&lt;String, Integer&gt; readRequest = ReadKeyValues.from("test-topic-header-filter", Integer.class)
        .with(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class)
        .filterOnHeaders(headersFilter)
        .build();

List&lt;KeyValue&lt;String, Integer&gt;&gt; consumedRecords = cluster.read(readRequest);</code></pre>
</div>
</div>
<div class="paragraph">
<p>It is possible to parameterize both <code>ReadKeyValues</code> and <code>ObserveKeyValues</code> with a headers filter. Like key and value filters, it is modelled using a <code>java.util.function.Predicate</code> on the target type <code>org.apache.kafka.common.header.Headers</code>. The default headers filter evaluates to <code>true</code> every time, so unless you do not explicitly provide a filter using <code>filterOnHeaders</code> like in the example, all consumed records pass the filter.</p>
</div>
<div class="paragraph">
<p>In this example, the filter only lets those records pass for which the header <code>aggregate</code> is set to the <code>String</code> <code>a</code>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Applying a header filter is also possible when observing a topic.
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Combining key, value and header filters is possible. Please note that in this case only such records that pass all filters are returned. Hence, conjoining key, value and header filters has <code>AND</code> semantics.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_obtaining_metadata_per_record">6.11. Obtaining metadata per-record</h3>
<div class="paragraph">
<p>An instance of <code>KeyValue</code> is associated with the optional type <code>KeyValueMetadata</code>. By default, this type is not set and thus <code>KeyValue::getMetadata</code> returns <code>Optional.empty</code>. Both <code>ReadKeyValues</code> and <code>ObserveKeyValues</code> provide a method called <code>includeMetadata</code> that explicitly enables metadata on a per-record basis. The listing underneath demonstrates this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">ObserveKeyValues&lt;String, String&gt; observeRequest = ObserveKeyValues.on("test-topic", 3)
        .includeMetadata()
        .build();

List&lt;KeyValue&lt;String, String&gt;&gt; records = cluster.observe(observeRequest);</code></pre>
</div>
</div>
<div class="paragraph">
<p>In this example, all instances of <code>KeyValue</code> feature an instance of <code>Optional&lt;KeyValueMetadata&gt;</code> which contains metadata for the resp. record. Metadata is currently limited to the coordinates of the record and thus closes over the 3-tuple <code>(topic, partition, offset)</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_seeking_to_a_dedicated_offset_of_a_topic_partition">6.12. Seeking to a dedicated offset of a topic-partition</h3>
<div class="paragraph">
<p>Consuming data continuously from topics that contain a huge amountof data may take quite some time, if a new consumer instance always starts to read from the beginning of the topic. To speed things up, you can skip to a dedicated offset for topic-partitions and start reading from there. The example underneath demonstrates how this is done when reading key-values using <code>ReadKeyValues</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">ReadKeyValues&lt;String, String&gt; readRequest = ReadKeyValues.from("test-topic").seekTo(0, 2).build();

List&lt;KeyValue&lt;String, String&gt;&gt; records = cluster.read(readRequest);</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Seeking is also a feature of <code>ObserveKeyValues</code> which means, that seeking to a dedicated offset is possible for all operations that a <code>RecordConsumer</code> provides.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="section:managing-topics">7. Managing topics</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Class <code>EmbeddedKafkaClusterRule</code> as well as <code>EmbeddedKafkaCluster</code> expose convenience methods for managing Kafka topics. Have a look at the <code>TopicManager</code> interface (Java omitted for brevity).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">public interface TopicManager {
    void createTopic(TopicConfig config);
    void deleteTopic(String topic);
    boolean exists(String topic);
    Map&lt;Integer, LeaderAndIsr&gt; fetchLeaderAndIsr(String topic);
    Properties fetchTopicConfig(String topic);
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Implementations of the <code>TopicCreator</code> interface currently use the <code>zkclient</code> library for topic management.</p>
</div>
<div class="paragraph">
<p>All operations are executed <strong>synchronously</strong>.</p>
</div>
<div class="sect2">
<h3 id="_creating_a_topic">7.1. Creating a topic</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">cluster.createTopic(TopicConfig.forTopic("test-topic").useDefaults());</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
By default, Kafka for JUnit enables the automatic creation of topics at the broker with defaults that should be sensible for local testing. However, if you find yourself in the situation to create a topic with a specific replication factor or number of partitions that deviate from their default setting, you should create that topic with the respective settings before writing the first Kafka record to it.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_deleting_a_topic">7.2. Deleting a topic</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">cluster.deleteTopic("test-topic");</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Deleting a topic will only set a deletion marker for that topic. The topic may not be deleted immediately after <code>deleteTopic</code> completes.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_determine_whether_a_topic_exists">7.3. Determine whether a topic exists</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">cluster.exists("test-topic");</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Returns <code>true</code> even if the topic is marked for deletion.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_retrieving_the_leader_and_the_in_sync_replica_set_isr">7.4. Retrieving the leader and the In-Sync-Replica Set (ISR)</h3>
<div class="paragraph">
<p>In case you have multiple brokers running and want to query their assignments and roles for a specific topic, you can use <code>TopicManager#fetchLeaderAndIsr</code> to retrieve that kind of information. The method returns an unmodifiable <code>java.util.Map</code> of <code>LeaderAndIsr</code> instances by their designated partition. The listing underneath shows how to retrieve this information for the topic named <code>test-topic</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">Map&lt;Integer, LeaderAndIsr&gt; leaderAndIsr = cluster.fetchLeaderAndIsr("test-topic");</code></pre>
</div>
</div>
<div class="paragraph">
<p>The type <code>LeaderAndIsr</code> is not to be confused with the same type in package <code>kafka.api</code>. The <code>LeaderAndIsr</code> implementation Kafka for JUnit is a simple transfer object that only contains the ID of the leader node and the IDs of all nodes that comprise the ISR.</p>
</div>
</div>
<div class="sect2">
<h3 id="_retrieving_the_topic_configuration_remotely">7.5. Retrieving the topic configuration remotely</h3>
<div class="paragraph">
<p>Looking up the topic configuration by accessing the cluster is easily done using the <code>TopicManager</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">Properties topicConfig = cluster.fetchTopicConfig("test-topic");</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="section:colophon">8. License</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This work is released under the terms of the Apache 2.0 license.</p>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Version 2.4.0<br>
 2020-02-07 20:02:00 CET
</div>
</div>
</body>
</html>
